{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19f4b00",
   "metadata": {},
   "source": [
    "# Dynamic & Static Inference Proof of Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0af0fff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fcd8554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\USER\\anaconda3\\envs\\ngt_test\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import joblib\n",
    "from collections import deque\n",
    "import itertools\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e80a98d",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10e0a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "STATIC_MODEL_PATH = '../models/ngt_static_classifier.pkl'\n",
    "DYNAMIC_MODEL_PATH = '../models/ngt_dynamic_classifier.pkl'\n",
    "\n",
    "ROLLING_WINDOW_SIZE = 5  # Number of frames for delta calculation\n",
    "\n",
    "STATIC_LETTERS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'I', 'K', 'L', 'M', \n",
    "                  'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "DYNAMIC_LETTERS = ['H', 'J', 'U', 'X', 'Z']\n",
    "\n",
    "# Add this to the configuration section at the top\n",
    "WRIST_MOTION_THRESHOLD = 0.1  # Threshold for detecting dynamic gestures\n",
    "WRIST_MOTION_WINDOW = 10       # Number of frames to track wrist movement\n",
    "MIN_CONFIDENCE_THRESHOLD = 70  # Minimum confidence to display prediction\n",
    "\n",
    "\n",
    "# Load models\n",
    "static_model = joblib.load(STATIC_MODEL_PATH)\n",
    "dynamic_model = joblib.load(DYNAMIC_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb6cbaf",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffffb4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LANDMARK PROCESSING (matches collection.py)\n",
    "# ============================================================\n",
    "\n",
    "def calc_landmark_list(image, landmarks):\n",
    "    \"\"\"Extract landmark coordinates from MediaPipe (x,y only).\"\"\"\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "    landmark_point = []\n",
    "    for landmark in landmarks.landmark:\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        landmark_point.append([landmark_x, landmark_y])\n",
    "    return landmark_point\n",
    "\n",
    "\n",
    "def pre_process_landmark(landmark_list):\n",
    "    \"\"\"Normalize landmarks to relative coordinates (matches collection.py).\"\"\"\n",
    "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "    \n",
    "    # Convert to relative coordinates (relative to wrist)\n",
    "    base_x, base_y = temp_landmark_list[0][0], temp_landmark_list[0][1]\n",
    "    for index, landmark_point in enumerate(temp_landmark_list):\n",
    "        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
    "        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
    "    \n",
    "    # Flatten\n",
    "    temp_landmark_list = list(itertools.chain.from_iterable(temp_landmark_list))\n",
    "    \n",
    "    # Normalization\n",
    "    max_value = max(list(map(abs, temp_landmark_list)))\n",
    "    if max_value > 0:\n",
    "        temp_landmark_list = list(map(lambda n: n / max_value, temp_landmark_list))\n",
    "    \n",
    "    return temp_landmark_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca2f0fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PREDICTION FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def predict_static(normalized_landmarks):\n",
    "    \"\"\"Predict static letter from 42 normalized features.\"\"\"\n",
    "    prediction_idx = static_model.predict([normalized_landmarks])[0]\n",
    "    probabilities = static_model.predict_proba([normalized_landmarks])[0]\n",
    "    confidence = max(probabilities) * 100\n",
    "    \n",
    "    # Convert index to letter\n",
    "    letter = chr(prediction_idx + ord('A'))\n",
    "    \n",
    "    return letter, confidence\n",
    "\n",
    "\n",
    "def predict_dynamic(landmark_buffer):\n",
    "    \"\"\"Predict dynamic letter from current frame + delta features.\"\"\"\n",
    "    if len(landmark_buffer) < ROLLING_WINDOW_SIZE:\n",
    "        return None, 0.0\n",
    "    \n",
    "    # Get current frame and frame from ROLLING_WINDOW_SIZE ago\n",
    "    current_landmarks = landmark_buffer[-1]\n",
    "    old_landmarks = landmark_buffer[0]\n",
    "    \n",
    "    # Compute delta features\n",
    "    delta_features = [curr - old for curr, old in zip(current_landmarks, old_landmarks)]\n",
    "    \n",
    "    # Concatenate: [current_42, delta_42] = 84 features\n",
    "    features = current_landmarks + delta_features\n",
    "    \n",
    "    prediction_idx = dynamic_model.predict([features])[0]\n",
    "    probabilities = dynamic_model.predict_proba([features])[0]\n",
    "    confidence = max(probabilities) * 100\n",
    "    \n",
    "    # Convert index to letter\n",
    "    letter = chr(prediction_idx + ord('A'))\n",
    "    \n",
    "    return letter, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e92f4d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wrist_motion(wrist_positions):\n",
    "    \"\"\"Calculate total wrist movement over the window.\"\"\"\n",
    "    if len(wrist_positions) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    total_distance = 0.0\n",
    "    positions = list(wrist_positions)\n",
    "    \n",
    "    for i in range(len(positions) - 1):\n",
    "        diff = np.array(positions[i+1]) - np.array(positions[i])\n",
    "        distance = np.linalg.norm(diff)\n",
    "        total_distance += distance\n",
    "    \n",
    "    return total_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95d575ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NGT Fingerspelling Recognizer\n",
      "Rolling window size: 5 frames\n",
      "Press 'q' to quit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MAIN LOOP\n",
    "# ============================================================\n",
    "\n",
    "# Setup MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Rolling buffer for normalized landmarks\n",
    "landmark_buffer = deque(maxlen=ROLLING_WINDOW_SIZE)\n",
    "wrist_buffer = deque(maxlen=WRIST_MOTION_WINDOW)\n",
    "\n",
    "# Setup camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 960)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 540)\n",
    "\n",
    "print(\"\\nNGT Fingerspelling Recognizer\")\n",
    "print(f\"Rolling window size: {ROLLING_WINDOW_SIZE} frames\")\n",
    "print(\"Press 'q' to quit\\n\")\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.5\n",
    ") as hands:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # Convert to RGB for MediaPipe\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            \n",
    "            # Draw hand skeleton\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, hand_landmarks, mp_hands.HAND_CONNECTIONS\n",
    "            )\n",
    "            \n",
    "            # Process landmarks\n",
    "            landmark_list = calc_landmark_list(frame, hand_landmarks)\n",
    "            normalized_landmarks = pre_process_landmark(landmark_list)\n",
    "            \n",
    "            # Track wrist position (landmark 0, use first 2 coords for motion)\n",
    "            wrist_landmark = hand_landmarks.landmark[0]\n",
    "            wrist_pos = [wrist_landmark.x, wrist_landmark.y, wrist_landmark.z]\n",
    "            wrist_buffer.append(wrist_pos)\n",
    "\n",
    "            # Calculate wrist motion\n",
    "            wrist_motion = calculate_wrist_motion(wrist_buffer)\n",
    "            \n",
    "            # Add to buffer\n",
    "            landmark_buffer.append(normalized_landmarks)\n",
    "            \n",
    "            # Predict static (always available)\n",
    "            static_letter, static_conf = predict_static(normalized_landmarks)\n",
    "            \n",
    "            # Predict dynamic (only if buffer is full)\n",
    "            if len(landmark_buffer) >= ROLLING_WINDOW_SIZE:\n",
    "                dynamic_letter, dynamic_conf = predict_dynamic(landmark_buffer)\n",
    "            else:\n",
    "                dynamic_letter, dynamic_conf = None, 0.0\n",
    "            \n",
    "            # SUPERVISOR DECISION LOGIC\n",
    "            is_dynamic_motion = wrist_motion > WRIST_MOTION_THRESHOLD\n",
    "            \n",
    "            if is_dynamic_motion and dynamic_letter is not None:\n",
    "                # Use dynamic prediction\n",
    "                final_letter = dynamic_letter\n",
    "                final_conf = dynamic_conf\n",
    "                source = \"DYNAMIC\"\n",
    "                color = (255, 0, 255)\n",
    "            else:\n",
    "                # Use static prediction\n",
    "                final_letter = static_letter\n",
    "                final_conf = static_conf\n",
    "                source = \"STATIC\"\n",
    "                color = (255, 255, 255)\n",
    "            \n",
    "            # Display final prediction (large)\n",
    "            cv2.putText(frame, final_letter,\n",
    "                       (50, 120), cv2.FONT_HERSHEY_SIMPLEX, 4, color, 8)\n",
    "            \n",
    "            cv2.putText(frame, f\"{final_conf:.0f}% - {source}\",\n",
    "                       (50, 170), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "            \n",
    "            # Debug panel with dark background\n",
    "            panel_x = 20\n",
    "            panel_y = 230\n",
    "            panel_width = 450\n",
    "            panel_height = 120\n",
    "            \n",
    "            # Dark background panel\n",
    "            overlay = frame.copy()\n",
    "            cv2.rectangle(overlay, (panel_x, panel_y), \n",
    "                         (panel_x + panel_width, panel_y + panel_height), \n",
    "                         (0, 0, 0), -1)\n",
    "            cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)\n",
    "            \n",
    "            # Debug text on dark background\n",
    "            y_pos = panel_y + 25\n",
    "            cv2.putText(frame, f\"Motion: {wrist_motion:.3f} | Threshold: {WRIST_MOTION_THRESHOLD:.3f}\",\n",
    "                       (panel_x + 10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
    "            y_pos += 30\n",
    "            \n",
    "            cv2.putText(frame, f\"Static:  {static_letter} ({static_conf:.0f}%)\",\n",
    "                       (panel_x + 10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)\n",
    "            y_pos += 30\n",
    "            \n",
    "            if dynamic_letter:\n",
    "                cv2.putText(frame, f\"Dynamic: {dynamic_letter} ({dynamic_conf:.0f}%)\",\n",
    "                           (panel_x + 10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 100, 255), 1)\n",
    "            else:\n",
    "                cv2.putText(frame, f\"Dynamic: waiting for buffer...\",\n",
    "                           (panel_x + 10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (100, 100, 100), 1)\n",
    "            y_pos += 30\n",
    "            \n",
    "            # Keybinding hints\n",
    "            cv2.putText(frame, \"k/j: threshold +/- 0.01 | K/J: +/- 0.05\",\n",
    "                       (panel_x + 10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (150, 150, 150), 1)\n",
    "            \n",
    "            # Motion bar (visual indicator)\n",
    "            bar_x = 20\n",
    "            bar_y = frame.shape[0] - 60\n",
    "            bar_width = 300\n",
    "            bar_height = 30\n",
    "            \n",
    "            # Background\n",
    "            cv2.rectangle(frame, (bar_x, bar_y), \n",
    "                         (bar_x + bar_width, bar_y + bar_height), \n",
    "                         (80, 80, 80), -1)\n",
    "            \n",
    "            # Fill (scale to 0.5 max for display)\n",
    "            max_display_motion = 0.3\n",
    "            fill_ratio = min(1.0, wrist_motion / max_display_motion)\n",
    "            fill_width = int(bar_width * fill_ratio)\n",
    "            \n",
    "            if is_dynamic_motion:\n",
    "                fill_color = (0, 255, 0)  # Green when above threshold\n",
    "            else:\n",
    "                fill_color = (100, 100, 100)  # Gray when below\n",
    "            \n",
    "            if fill_width > 0:\n",
    "                cv2.rectangle(frame, (bar_x, bar_y), \n",
    "                             (bar_x + fill_width, bar_y + bar_height), \n",
    "                             fill_color, -1)\n",
    "            \n",
    "            # Threshold line (red)\n",
    "            threshold_x = bar_x + int(bar_width * (WRIST_MOTION_THRESHOLD / max_display_motion))\n",
    "            cv2.line(frame, (threshold_x, bar_y), \n",
    "                    (threshold_x, bar_y + bar_height), \n",
    "                    (0, 0, 255), 2)\n",
    "            \n",
    "            # Border\n",
    "            cv2.rectangle(frame, (bar_x, bar_y), \n",
    "                         (bar_x + bar_width, bar_y + bar_height), \n",
    "                         (200, 200, 200), 2)\n",
    "        \n",
    "        else:\n",
    "            # No hand detected - clear buffers\n",
    "            landmark_buffer.clear()\n",
    "            wrist_buffer.clear()\n",
    "            cv2.putText(frame, \"Show hand\",\n",
    "                       (50, 120), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 4)\n",
    "        \n",
    "        cv2.imshow('NGT Recognizer', frame)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        # Vim-style threshold adjustment\n",
    "        if key == ord('k'):\n",
    "            WRIST_MOTION_THRESHOLD += 0.01\n",
    "            print(f\"Threshold increased to {WRIST_MOTION_THRESHOLD:.3f}\")\n",
    "        elif key == ord('j'):\n",
    "            WRIST_MOTION_THRESHOLD = max(0.0, WRIST_MOTION_THRESHOLD - 0.01)\n",
    "            print(f\"Threshold decreased to {WRIST_MOTION_THRESHOLD:.3f}\")\n",
    "        elif key == ord('K'):\n",
    "            WRIST_MOTION_THRESHOLD += 0.05\n",
    "            print(f\"Threshold increased to {WRIST_MOTION_THRESHOLD:.3f}\")\n",
    "        elif key == ord('J'):\n",
    "            WRIST_MOTION_THRESHOLD = max(0.0, WRIST_MOTION_THRESHOLD - 0.05)\n",
    "            print(f\"Threshold decreased to {WRIST_MOTION_THRESHOLD:.3f}\")\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7812814d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ngt_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
